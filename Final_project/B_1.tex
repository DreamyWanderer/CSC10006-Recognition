\section*{B-1. Robust Text-Independent Speaker Identification Using Gaussian Mixture Speaker Models}

This section is based mainly on \cite{Reynolds1995}. All formula, information, specifications, etc. are collected from those references. Any other references beside those will be cited in-text directly.

\subsection*{Overview}

This model is proposed by Ph.D. Douglas A. Reynolds (a senior member of technical staff at MIT Lincoln Laboratory, also a memmber of IEEE) and Richard C. Rose. It was first published in IEEE Transactions on Speech and Audio processsing (Vol. 3, No. 1), in January 1995. This was one of most famous unsupervised machine learning model used in speaker recognition, and are still used even to this day.

\subsection*{Some applications}

\begin{enumerate}
	
	\item \emph{Text-Independent speaker identification}.
	
	 GMM can capture the underlying distribution of spectral in voice of each person, which is charactersized by his/her vocal tract shape. As a result, GMM does not need information of sematic to distiguish person's voice. Each speaker registered to the databases and supplied their voice sample will have their own classifier as GMM. 
	\item \emph{3D face recognition}. 
	
	Using only a single sensor, region covariance matrixes derives low-dimensional feature vectors, which finally modeled by GMM. They are finally then fed to a SVM to retrieve the identity.
	
	\item \emph{Classify the pixel distribution of main brain tissues}. 
	
	Here GMM is used to study the important variability in the mixing probability associated with white matter and grey matter between the left and right hemispheres.

\end{enumerate}

\subsection*{Gaussian Mixture Model}

Following are the main steps to train a common GMM (in this problem):

\begin{description}

	\item[Step 1] \emph{Define the GMM}
	
	Assume that a feature vector respects to an arbitary voice is generated through the following probabilistic process:
	
	\begin{equation*}
		p(\vec{x} \vert \lambda) = \sum_{i = 1}^K p_ib_i(\vec{x})
	\end{equation*}
	
	where $\vec{x}$ is a $m$-dimensional vector (or a feature vector), $b_i(\vec{x})$ is a component density, $p_i$ is the respective mixture weight. Each component densitie $b_i $have a mean $\vec{\mu_i}$ and a covariance matrix $\bm{\Sigma_i}$. This complete Gaussian mixture density is parameterized by $\lambda = \{p_i, \vec{\mu_i}, \bm{\Sigma_i} \}$.
	
	The intuition is that each speark will has their own $\lambda$, or a Gaussian mixture.
	
	\item[Step 2] \emph{Retrieve the feature vector}
	
	From each utterance, we use MFCC (Mel Frequency Cepstral Coefficient) to retrieve feature vectors. This vector contains information about speactral (frequency range, time, etc.) of important characteristics produced solely by vocal tract of a speaker.
	
	\item[Step 3] \emph{Choose number of clusters}
	
	Choose a suitable value for $K$.
	
	\item[Step 4] \emph{Training using Expectation-Maximization (EM) algorithm} 
	
	First, EM initializes $\lambda$ randomly, then it repeats two steps until convergence: (1) \emph{Expectation step}: for each instance, the algorithm estimates the probability that it belongs to each cluster, using current cluster $\lambda$. (2) \emph{Maximization step} Each cluster is updated using all feature vectors, with each instance weighted by the estimated probability that it belongs to that cluster. It means $\lambda$ is affected most by instaces have high chance generated by that cluster.
	
	\item[Step 5] \emph{Speaker identification}
	
	A gropu of $S$ speaker is represented by GMM's $\lambda_1, \lambda_2, \ldots, \lambda_S$. In the inference step, given a utterance of a speaker registered before, we will determine:
	
	\begin{equation*}
		\hat{S} = \max_{1 \leq k \leq S} \sum_{t = 1}^{T} \log p(\vec{x}_t \vert \lambda_k)
	\end{equation*}
	
	where $\hat{S}$ is index of speaker, $\vec{t}_t$ is feature vector retrieved from speaker utterance. $p(\vec{x}_t \vert \lambda_k)$ can be calculated easily by using formula in \textbf{Step 1} with trained $\lambda$.
		
\end{description}

\subsection*{Evaluation}

The evaluation is shown as Table \ref{tab:GMM1}.

\begin{table}[ht!]
	\centering
	
	\begin{tabularx}{1.0\textwidth}{ XL{0.3\textwidth}L{0.3\textwidth} }
	
	\toprule
	\textbf{Property} &  \textbf{Advantages} & \textbf{Disadvantages} \\
	
	\midrule

	Accuracy & High in case normalization, the data has some form of Gaussian distribution. Good for even huge population size & Easy to be affected by outliners, noisy, environmental context. Cannot cope to nonellipsoidal clusters or arbitary shapes. \\ \addlinespace[2em]
	
	Running time & Enormous quick. \\ \addlinespace[2em]
	
	Memory usage & Low. \\ \addlinespace[2em]
	
	Library support & Various. Out-of-the-box usage and easy tuning. \\ \addlinespace[2em]
	
	Architecture & Simple, easy to visualize and interpret. & Need to determine suitable number of cluster.\\
	
	\bottomrule
	\end{tabularx}
	
	\caption{Evaluate GMM in this speaker recognition system}
	\label{tab:GMM1}

\end{table}
